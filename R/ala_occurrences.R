#' Download occurrence data from the ALA
#'
#' @param taxon_id string: single species ID or vector of species ids. Use
#' `ala_taxa()` to get lookup species id.
#' @param filters data.frame: generated by `ala_filters()`
#' @param area string or sf object: restrict the search to an area. Can provide
#' sf object, or a wkt string. WKT strings longer than 10000 characters will
#' not be accepted by the ALA- see the vignette for how to work around this.
#' @param columns string: vector of columns to return in download.
#' @param generate_doi logical: by default no DOI will be generated. Set to
#' true if you intend to use the data in a publication or similar. If
#' generated, DOI is stored in attributes.
#' @param email string: the email address of the user performing the download
#' (required unless \code{record_count_only = TRUE}
#' @param email_notify logical: set to `FALSE` by default, set to true if you
#' would like an email notification for the download
#' @param caching string: should the results be cached? Either "on" or "off"
#' @examples
#' \dontrun{
#' ## Retrieve all machine-observed reptile records in Victoria in the past
#' ## five years, with the default ALA data quality profile
#' id <- ala_taxa("Reptilia")$taxon_concept_id
#' occ <- ala_occurrences(taxon_id = id, filters =
#' ala_filters(list(year = seq(2015, 2020), state = "Victoria")))
#' }
#' @export ala_occurrences

ala_occurrences <- function(taxon_id, filters, area,
                            columns = "default",
                            email = "ala4r@ala.org.au", generate_doi = FALSE,
                            email_notify = FALSE,
                            caching = "off") {

  assert_that(is.flag(generate_doi))
  assert_that(is.flag(email_notify))
  assert_that(is.character(email))

  # is it worth validating the email with a regex? this won't 
  # be able to tell if the email is registered or not

  query <- list()

  if (missing(taxon_id) & missing(filters) & missing(area)) {
    # stop or allow users to download the whole ALA?
    stop("Need to provide one of `taxon id`, `filters` or `area`")
  }

  if(!missing(taxon_id)) {
    # should species id be validated?
    assert_that(is.character(taxon_id))
    taxa_query <- build_taxa_query(taxon_id)
  } else {
    taxa_query <- NULL
  }

  # validate filters
  if (!missing(filters)) {
    assert_that(is.data.frame(filters))
    validate_filters(filters)
    filter_query <- build_filter_query(filters)
  } else {
    filter_query <- NULL
  }
  if (!is.null(filter_query) || !is.null(taxa_query)) {
    query$fq <- paste0("(", paste(c(taxa_query, filter_query), collapse = ' AND '), ")")
  }
  

  # not yet released
  #query$qualityProfile <- 'ALA'

  # Two issues with area validation: 
  # wellknown validates wkt correctly?? 
  # 504 error is returned from the server if a polygon is too complicated
  if (!missing(area)) {
    # convert area to wkt if not already
    query$wkt <- build_area_query(area)
  }

  # do a occurrence count query first? to warn the user

  if (sum(nchar(query$fq), nchar(query$wkt), na.rm = TRUE) > 1948) {
    qid <- cache_params(query)
    query <- list(q = paste0("qid:",qid))
  }

  # handle caching
  # look for file to download- if it doesn't exist, continue on
  # use base_url_biocache as the filename? otherwise won't be found
  cache_file <- cache_filename(url = getOption("ALA4R_server_config")$
                                   base_url_biocache,
                               path = "ws/occurrences/offline/download",
                               params = query, ext = ".zip")

  if (caching == "on" & file.exists(cache_file)) {
    message("Using existing file")
    # look for file using query parameters
    data <- read.csv(unz(cache_file, "data.csv"), stringsAsFactors = FALSE)
    # if file doesn't exist, continue as before
    return(data)
  }
  message("Caching is ", caching)
  message("Download path is ", cache_file)

  count <- record_count(query)
  check_count(count)
  

  # Add columns after getting record count
  if (missing(columns)) {
    message("No columns specified, default columns will be returned.")
    columns <- "default"
  }
  query$fields <- build_columns(columns)

  if (generate_doi) {
    query$mintDoi <- "true"
  }

  if (isFALSE(email_notify)) {
    query$emailNotify <- "false"
  }

  # Get data
  url <- getOption("ALA4R_server_config")$base_url_biocache
  query <- c(query, email = email, reasonTypeId = 10, dwcHeaders = "true",
                 qa = "none")
  
  download_path <- wait_for_download(url, query)
  data_path <- ala_download(url = "https://biocache.ala.org.au",
                       path = download_path,
                       cache_file = cache_file, ext = ".zip")
  df <- read.csv(unz(data_path, "data.csv"), stringsAsFactors = FALSE)
  # add DOI as attribute
  doi <- NA
  if (generate_doi) {
    try({
      doi_file <- read.table(unz(data_path, "doi.txt"))
      doi <- as.character(doi_file$V1)
      attr(df, "doi") <- doi},
      silent = TRUE)
  }
  
  if (generate_doi && is.na(doi)) {
    warning("No DOI was generated for download. The DOI server may
                        be down. Please try again later")
  }
  
  return(df)
}

# check validity of fields?
# maybe only warn because it is possible to get results for fields not in ala_fields
# should the fields you have in the filters also be included?

build_columns <- function(cols) {
  presets <- c("default")
  default_columns <- c("latitude", "longitude","occurrence_date","taxon_name",
                       "taxon_concept_lsid", "id", "data_resource")
  additional_cols <- cols[!cols %in% presets]
  preset_selected <- cols[cols %in% presets]
    
  if (length(preset_selected) > 1) {
    stop("Only one preset option can be specified in `columns`")
  }
  else if (length(preset_selected) == 1) {
    preset_cols <- switch(preset_selected,
           "default" = default_columns)
    fields <- c(preset_cols, additional_cols)
  }
  else {
    fields <- cols
  }
  paste0(fields, collapse = ",")
}

#' Build dataframe of columns to keep
#' @param cols string: vector of column names to include. If a col matches a
#' group name, columns in that group will be added to the list
#' @export ala_columns
ala_columns <- function(cols) {
  presets <- c("basic", "event")
  additional_cols <- cols[!cols %in% presets]
  preset <- sapply(cols[cols %in% presets], function(x) {
    preset_cols(x)
  }, USE.NAMES = FALSE)
  c(unlist(preset), additional_cols)
}



preset_cols <- function(type) {
  cols <- switch (type,
    "basic" = c("latitude", "longitude","occurrence_date","taxon_name",
                "taxon_concept_lsid", "id", "data_resource"),
    "event" = c("event_remarks", "event_time", "event_id", "occurrence_date",
                "sampling_effort", "sampling_protocol") 
  )
  cols
  
}

wait_for_download <- function(url, query) {
  status <- ala_GET(url, "ws/occurrences/offline/download",
                    params = query)
  
  status_url <- parse_url(status$statusUrl)
  status <- ala_GET(url, path = status_url$path)
  while (tolower(status$status) %in% c("inqueue", "running")) {
    status <- ala_GET(url, path = status_url$path)
    Sys.sleep(2)
  }
  parse_url(status$downloadUrl)$path
}

check_count <- function(count) {
  message("This query will return ", count, " records")
  
  if (count == 0) {
    # should this return something?
    stop()
  } else if (count > 50000000) {
    stop("A maximum of 50 million records can be retrieved at once.",
         " Please narrow the query and try again.")
  }
}