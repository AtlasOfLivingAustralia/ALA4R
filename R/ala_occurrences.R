#' Download occurrence data from the ALA
#'
#' @param taxon_id string: single species ID or vector of species ids. Use
#' `ala_taxa()` to get lookup species id.
#' @param filters data.frame: generated by `ala_filters()`
#' @param area string or sf object: restrict the search to an area. Can provide
#' sf object, or a wkt string. WKT strings longer than 10000 characters will
#' not be accepted by the ALA- see the vignette for how to work around this.
#' @param columns string: vector of columns to return in download.
#' @param generate_doi logical: by default no DOI will be generated. Set to
#' true if you intend to use the data in a publication or similar. If
#' generated, DOI is stored in attributes.
#' @param email string: the email address of the user performing the download
#' (required unless \code{record_count_only = TRUE}
#' @param email_notify logical: set to `FALSE` by default, set to true if you
#' would like an email notification for the download
#' @param caching string: should the results be cached? Either "on" or "off"
#' @examples
#' \dontrun{
#' ## Retrieve all machine-observed reptile records in Victoria in the past
#' ## five years, with the default ALA data quality profile
#' id <- ala_taxa("Reptilia")$taxon_concept_id
#' occ <- ala_occurrences(taxon_id = id, filters =
#' ala_filters(list(year = seq(2015, 2020), state = "Victoria")))
#' }
#' @export ala_occurrences

ala_occurrences <- function(taxon_id, filters, area,
                            columns = ala_columns("basic"),
                            email = "ala4r@ala.org.au", generate_doi = FALSE,
                            email_notify = FALSE,
                            caching = "off") {

  assert_that(is.flag(generate_doi))
  assert_that(is.flag(email_notify))
  assert_that(is.character(email))

  # is it worth validating the email with a regex? this won't 
  # be able to tell if the email is registered or not

  query <- list()

  if (missing(taxon_id) & missing(filters) & missing(area)) {
    # stop or allow users to download the whole ALA?
    stop("Need to provide one of `taxon id`, `filters` or `area`")
  }

  if(!missing(taxon_id)) {
    # should species id be validated?
    if (inherits(taxon_id, "data.frame") &&
        "taxon_concept_id" %in% colnames(taxon_id)) {
      taxon_id <- taxon_id$taxon_concept_id
    }
    assert_that(is.character(taxon_id))
    taxa_query <- build_taxa_query(taxon_id)
  } else {
    taxa_query <- NULL
  }

  # validate filters
  if (!missing(filters)) {
    assert_that(is.data.frame(filters))
    validate_filters(filters)
    filters$name <- dwc_to_ala(filters$name)
    filter_query <- build_filter_query(filters)
  } else {
    filter_query <- NULL
  }
  if (!is.null(filter_query) || !is.null(taxa_query)) {
    query$fq <- paste0("(", paste(c(taxa_query, filter_query), collapse = ' AND '), ")")
  }
  

  # not yet released
  #query$qualityProfile <- 'ALA'

  # Two issues with area validation: 
  # wellknown validates wkt correctly?? 
  # 504 error is returned from the server if a polygon is too complicated
  if (!missing(area)) {
    # convert area to wkt if not already
    query$wkt <- build_area_query(area)
  }

  # do a occurrence count query first? to warn the user

  if (sum(nchar(query$fq), nchar(query$wkt), na.rm = TRUE) > 1948) {
    qid <- cache_params(query)
    query <- list(q = paste0("qid:",qid))
  }

  # handle caching
  # look for file to download- if it doesn't exist, continue on
  # use base_url_biocache as the filename? otherwise won't be found
  cache_file <- cache_filename(url = getOption("ALA4R_server_config")$
                                   base_url_biocache,
                               path = "ws/occurrences/offline/download",
                               params = query, ext = ".zip")

  if (caching == "on" & file.exists(cache_file)) {
    message("Using existing file")
    # look for file using query parameters
    data <- read.csv(unz(cache_file, "data.csv"), stringsAsFactors = FALSE)
    # if file doesn't exist, continue as before
    return(data)
  }
  #message("Caching is ", caching)
  #message("Download path is ", cache_file)

  count <- record_count(query)
  check_count(count)
  

  # Add columns after getting record count
  if (missing(columns)) {
    message("No columns specified, default columns will be returned.")
    columns <- ala_columns("basic")
  } else {
    query$fields <- build_columns(columns[columns$type != "assertion",])
    query$qa <- build_columns(columns[columns$type == "assertion",])
  }

  if (generate_doi) {
    query$mintDoi <- "true"
  }

  if (isFALSE(email_notify)) {
    query$emailNotify <- "false"
  }

  # Get data
  url <- getOption("ALA4R_server_config")$base_url_biocache
  query <- c(query, email = email, reasonTypeId = 10, dwcHeaders = "true")
  
  download_path <- wait_for_download(url, query)
  data_path <- ala_download(url = "https://biocache.ala.org.au",
                       path = download_path,
                       cache_file = cache_file, ext = ".zip")
  df <- read.csv(unz(data_path, "data.csv"), stringsAsFactors = FALSE)
  
  # rename cols so they match requested cols
  names(df) <- rename_columns(names(df), type = "occurrence")
  # add DOI as attribute
  doi <- NA
  if (generate_doi) {
    try({
      doi_file <- read.table(unz(data_path, "doi.txt"))
      doi <- as.character(doi_file$V1)
      attr(df, "doi") <- doi},
      silent = TRUE)
  }
  
  if (generate_doi && is.na(doi)) {
    warning("No DOI was generated for download. The DOI server may
                        be down. Please try again later")
  }
  
  return(df)
}

# check validity of fields?
# maybe only warn because it is possible to get results for fields not in ala_fields
# should the fields you have in the filters also be included?

build_columns <- function(col_df) {
  if (nrow(col_df) == 0) {
    return("")
  }
  ala_cols <- dwc_to_ala(col_df$name)
  paste0(ala_cols, collapse = ",")
}


#' Build dataframe of columns to keep
#' @param group string: name of column group to include
#' @param extra string: additional column names to return
#' @export ala_columns
ala_columns <- function(group, extra) {
  
  if (!missing(group)) {
    group_cols <- data.table::rbindlist(lapply(group, function(x) {
      data.frame(name = preset_cols(x), type = "field",
                 stringsAsFactors = FALSE)
    }))} else {
      group_cols <- NULL
    }
  
  
  assertions <- ala_fields("assertion")$name
  if (!missing(extra)) {
   extra_cols <- data.table::rbindlist(lapply(extra, function(x) {
     type <- ifelse(x %in% assertions, "assertion", "field")
    data.frame(name = x, type = type, stringsAsFactors = FALSE)
   }))} else {
     extra_cols <- NULL
   }
  
  all_cols <- rbind(group_cols, extra_cols)
  # remove duplicates
  all_cols[!duplicated(all_cols$name),]
}



preset_cols <- function(type) {
  valid_groups <- c("basic", "event")
  # use ALA version of taxon name to avoid ambiguity (2 fields map to dwc name)
  cols <- switch (type,
    "basic" = c("decimalLatitude", "decimalLongitude","eventDate","taxon_name",
                "taxonConceptID", "recordID", "data_resource"),
    "event" = c("eventRemarks", "eventTime", "eventID", "eventDate",
                "samplingEffort", "samplingProtocol") ,
    stop("\"", type, "\" is not a valid column group. Valid groups are: ",
         paste(valid_groups, collapse = ", "))
  )
  cols
  
}


wait_for_download <- function(url, query) {
  status <- ala_GET(url, "ws/occurrences/offline/download",
                    params = query)
  
  status_url <- parse_url(status$statusUrl)
  status <- ala_GET(url, path = status_url$path)
  while (tolower(status$status) %in% c("inqueue", "running")) {
    status <- ala_GET(url, path = status_url$path)
    Sys.sleep(2)
  }
  parse_url(status$downloadUrl)$path
}

check_count <- function(count) {
  if (count == 0) {
    stop("This query does not match any records.")
  } else if (count > 50000000) {
    stop("A maximum of 50 million records can be retrieved at once.",
         " Please narrow the query and try again.")
  } else {
    message("This query will return ", count, " records")
  }
}